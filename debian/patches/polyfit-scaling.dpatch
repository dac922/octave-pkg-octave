#! /bin/sh /usr/share/dpatch/dpatch-run
## -*- diff -*-
## polyfit-scaling.dpatch by Rafael Laboissiere <rafael@debian.org>
##
## DP: Introduce polyfit.m from the Mercurial repository (2008-01-03)
## DP: with new algorithm for rescaling the input values, which
## DP: improves stability. Fixes Bug#510577.

@DPATCH@
diff -urNad octave3.0-3.0.3~/scripts/polynomial/polyfit.m octave3.0-3.0.3/scripts/polynomial/polyfit.m
--- octave3.0-3.0.3~/scripts/polynomial/polyfit.m	2008-09-24 09:13:48.000000000 +0200
+++ octave3.0-3.0.3/scripts/polynomial/polyfit.m	2009-01-04 13:21:23.000000000 +0100
@@ -18,30 +18,17 @@
 ## <http://www.gnu.org/licenses/>.
 
 ## -*- texinfo -*-
-## @deftypefn {Function File} {[@var{p}, @var{s}] =} polyfit (@var{x}, @var{y}, @var{n})
+## @deftypefn {Function File} {[@var{p}, @var{s}, @var{mu}] =} polyfit (@var{x}, @var{y}, @var{n})
 ## Return the coefficients of a polynomial @var{p}(@var{x}) of degree
-## @var{n} that minimizes
-## @iftex
-## @tex
-## $$
-## \sum_{i=1}^N (p(x_i) - y_i)^2
-## $$
-## @end tex
-## @end iftex
-## @ifinfo
-## @code{sumsq (p(x(i)) - y(i))},
-## @end ifinfo
-##  to best fit the data in the least squares sense.
+## @var{n} that minimizes the least-squares-error of the fit.
 ##
 ## The polynomial coefficients are returned in a row vector.
 ##
-## If two output arguments are requested, the second is a structure
-## containing the following fields:
+## The second output is a structure containing the following fields:
 ##
-## @table @code
+## @table @samp
 ## @item R
-## The Cholesky factor of the Vandermonde matrix used to compute the
-## polynomial coefficients.
+## Triangular factor R from the QR decomposition.
 ## @item X
 ## The Vandermonde matrix used to compute the polynomial coefficients.
 ## @item df
@@ -51,6 +38,17 @@
 ## @item yf
 ## The values of the polynomial for each value of @var{x}.
 ## @end table
+##
+## The second output may be used by @code{polyval} to calculate the 
+## statistical error limits of the predicted values.
+##
+## When the third output, @var{mu}, is present the 
+## coefficients, @var{p}, are associated with a polynomial in
+## @var{xhat} = (@var{x}-@var{mu}(1))/@var{mu}(2).
+## Where @var{mu}(1) = mean (@var{x}), and @var{mu}(2) = std (@var{x}).
+## This linear transformation of @var{x} improves the numerical
+## stability of the fit.
+## @seealso{polyval, residue}
 ## @end deftypefn
 
 ## Author: KH <Kurt.Hornik@wu-wien.ac.at>
@@ -59,12 +57,17 @@
 
 function [p, s, mu] = polyfit (x, y, n)
 
-
-  if (nargin != 3)
+  if (nargin < 3 || nargin > 4)
     print_usage ();
   endif
 
-  if (! (isvector (x) && isvector (y) && size_equal (x, y)))
+  if (nargout > 2)
+    ## Normalized the x values.
+    mu = [mean(x), std(x)];
+    x = (x - mu(1)) / mu(2);
+  endif
+
+  if (! size_equal (x, y))
     error ("polyfit: x and y must be vectors of the same size");
   endif
 
@@ -74,17 +77,21 @@
 
   y_is_row_vector = (rows (y) == 1);
 
-  l = length (x);
+  ## Reshape x & y into column vectors.
+  l = numel (x);
   x = reshape (x, l, 1);
   y = reshape (y, l, 1);
 
-  X = (x * ones (1, n+1)) .^ (ones (l, 1) * (n : -1 : 0));
+  ## Construct the Vandermonde matrix.
+  v = (x * ones (1, n+1)) .^ (ones (l, 1) * (n : -1 : 0));
 
-  p = X \ y;
+  ## Solve by QR decomposition.
+  [q, r, k] = qr (v, 0);
+  p = r \ (y' * q)';
+  p(k) = p;
 
   if (nargout > 1)
-
-    yf = X*p;
+    yf = v*p;
 
     if (y_is_row_vector)
       s.yf = yf.';
@@ -92,15 +99,73 @@
       s.yf = yf;
     endif
 
-    [s.R, dummy] = chol (X'*X);
-    s.X = X;
+    s.R = r;
+    s.X = v;
     s.df = l - n - 1;
     s.normr = norm (yf - y);
-
   endif
 
-  ## Return value should be a row vector.
-
+  ## Return a row vector.
   p = p.';
 
 endfunction
+
+%!test
+%! x = [-2, -1, 0, 1, 2];
+%! assert(all (all (abs (polyfit (x, x.^2+x+1, 2) - [1, 1, 1]) < sqrt (eps))));
+
+%!error(polyfit ([1, 2; 3, 4], [1, 2, 3, 4], 2))
+
+%!test
+%! x = [-2, -1, 0, 1, 2];
+%! assert(all (all (abs (polyfit (x, x.^2+x+1, 3) - [0, 1, 1, 1]) < sqrt (eps))));
+
+%!test
+%! x = [-2, -1, 0, 1, 2];
+%! fail("polyfit (x, x.^2+x+1)");
+
+%!test
+%! x = [-2, -1, 0, 1, 2];
+%! fail("polyfit (x, x.^2+x+1, [])");
+
+## Test difficult case where scaling is really needed. This example
+## demonstrates the rather poor result which occurs when the dependent
+## variable is not normalized properly.
+## Also check the usage of 2nd & 3rd output arguments.
+%!test
+%! x = [ -1196.4, -1195.2, -1194, -1192.8, -1191.6, -1190.4, -1189.2, -1188, \
+%!       -1186.8, -1185.6, -1184.4, -1183.2, -1182];
+%! y = [ 315571.7086, 315575.9618, 315579.4195, 315582.6206, 315585.4966,    \
+%!       315588.3172, 315590.9326, 315593.5934, 315596.0455, 315598.4201,    \
+%!       315600.7143, 315602.9508, 315605.1765 ];
+%! [p1, s1] = polyfit (x, y, 10);
+%! [p2, s2, mu] = polyfit (x, y, 10);
+%! assert (2*s2.normr < s1.normr)
+
+%!test
+%! x = 1:4;
+%! p0 = [1i, 0, 2i, 4];
+%! y0 = polyval (p0, x);
+%! p = polyfit (x, y0, numel(p0)-1);
+%! assert (p, p0, 1000*eps)
+
+%!test
+%! x = 1000 + (-5:5);
+%! xn = (x - mean (x)) / std (x);
+%! pn = ones (1,5);
+%! y = polyval (pn, xn);
+%! [p, s, mu] = polyfit (x, y, numel(pn)-1);
+%! [p2, s2] = polyfit (x, y, numel(pn)-1);
+%! assert (p, pn, s.normr)
+%! assert (s.yf, y, s.normr)
+%! assert (mu, [mean(x), std(x)])
+%! assert (s.normr/s2.normr < sqrt(eps))
+
+%!test
+%! x = [1, 2, 3; 4, 5, 6];
+%! y = [0, 0, 1; 1, 0, 0];
+%! p = polyfit (x, y, 5);
+%! expected = [0, 1, -14, 65, -112, 60]/12;
+%! assert (p, expected, sqrt(eps))
+
+
diff -urNad octave3.0-3.0.3~/test/test_poly.m octave3.0-3.0.3/test/test_poly.m
--- octave3.0-3.0.3~/test/test_poly.m	2008-09-24 09:13:50.000000000 +0200
+++ octave3.0-3.0.3/test/test_poly.m	2009-01-04 13:22:01.000000000 +0100
@@ -109,9 +109,6 @@
 %! x = [-2, -1, 0, 1, 2];
 %! assert(all (all (abs (polyfit (x, x.^2+x+1, 3) - [0, 1, 1, 1]) < sqrt (eps))));
 
-%% test/octave.test/poly/polyfit-3.m
-%!error polyfit ([1, 2; 3, 4], [1, 2; 3, 4], 4);
-
 %% test/octave.test/poly/polyfit-4.m
 %!test
 %! x = [-2, -1, 0, 1, 2];
